
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>tensorflow学习-word2vec | 山上掏金</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    <meta name="baidu-site-verification" content="Poq2hMEF9U" />
    
    <meta name="author" content="wenxi">
    

    
    <meta name="description" content="tensorflow学习笔记系列原始内容，可从CS 20SI: Tensorflow for Deep Learning Research。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。 word2vec词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相">
<meta name="keywords" content="笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow学习-word2vec">
<meta property="og:url" content="https://chlyzzo.github.io/2017/09/28/tensorflow学习-word2vec/index.html">
<meta property="og:site_name" content="山上掏金">
<meta property="og:description" content="tensorflow学习笔记系列原始内容，可从CS 20SI: Tensorflow for Deep Learning Research。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。 word2vec词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相">
<meta property="og:updated_time" content="2017-10-27T07:21:12.761Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tensorflow学习-word2vec">
<meta name="twitter:description" content="tensorflow学习笔记系列原始内容，可从CS 20SI: Tensorflow for Deep Learning Research。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。 word2vec词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相">

    
    <link rel="alternative" href="/atom.xml" title="山上掏金" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/logo.png">
    <link rel="apple-touch-icon-precomposed" href="/img/logo.png">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_pv">
    站浏览量<span id="busuanzi_value_site_pv"></span>次
  </span>
  <span id="busuanzi_container_site_uv">
    站访问人数<span id="busuanzi_value_site_uv"></span>
  </span>
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="山上掏金">山上掏金</a></h1>
				<h2 class="blog-motto">每天早上起床就是为了比昨天更快乐，掏金者的一天是新的开始.</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/about">作者</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:chlyzzo.github.io">
					</form>
					
					</li>
				</ul>
			</nav>
</div>

    </header>
    <div id="container">
      <div id="main" class="post moveMain" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/09/28/tensorflow学习-word2vec/" title="tensorflow学习-word2vec" itemprop="url">tensorflow学习-word2vec</a>
  </h1>
  <div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/笔记/">笔记</a>
  </div>

</div>


  <p class="article-author">By
       
		<a href="/about" title="wenxi" target="_blank" itemprop="author">wenxi</a>
		
  <p class="article-time">
    <time datetime="2017-09-28T07:05:07.000Z" itemprop="datePublished"> 2017-09-28 阅读量 <span id="busuanzi_value_page_pv"></span></time>
  </p>
</header>

	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">文章目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#word2vec"><span class="toc-number">1.</span> <span class="toc-text">word2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skip-model训练过程"><span class="toc-number">2.</span> <span class="toc-text">skip-model训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nce-loss"><span class="toc-number">2.1.</span> <span class="toc-text">nce_loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练步骤"><span class="toc-number">2.2.</span> <span class="toc-text">训练步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-number">3.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#应用"><span class="toc-number">4.</span> <span class="toc-text">应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型复用"><span class="toc-number">5.</span> <span class="toc-text">模型复用</span></a></li></ol>
		
		</div>
		
		<p>tensorflow学习笔记系列原始内容，可从<a href="http://web.stanford.edu/class/cs20si/syllabus.html" target="_blank" rel="external">CS 20SI: Tensorflow for Deep Learning Research</a>。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相当的有one-hot编码，即0/1标示；word2vec的理解是把词映射到一个多维空间内，相同或相似的词会在空间内离得很近。这种空间映射在不同语言的翻译中体现在，意思相同的词在各自语言的空间所处的位置是一样的。</p>
<p>总之，词嵌入的word2vec，是把词映射成一维向量，后续的应用可以基于向量来操作。</p>
<p>训练word2vec的方法有很多，两种常见的大类模型是skip-model和cbow-model；在skip-model中，是基于中心词得到其前后skip个词，组成词对作为训练样本；而cbow是在中心词的前后c个词与中心组成一个对作为正样本，而非中心词的都是负样本，NEG(w)。这里对cbow不细讲，主要讲skip-model的训练。</p>
<h2 id="skip-model训练过程"><a href="#skip-model训练过程" class="headerlink" title="skip-model训练过程"></a>skip-model训练过程</h2><p>首先，必须了解使用模型训练，要先给定样本、参数、损失函数、优化器。skip-model中样本是中心词前后skip个词构成的词对，参数是神经网络的权重，损失函数用nce，优化器是批梯度下降法。除了样本需要自定义生成外，其他的在tensorflow中均有封装函数可使用，这里会着重讲下nce损失函数，只是个人阅读后的见解。</p>
<h3 id="nce-loss"><a href="#nce-loss" class="headerlink" title="nce_loss"></a>nce_loss</h3><p>噪声对比估计损失，可想看<a href="http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf" target="_blank" rel="external">论文</a>,详见tensorflow的<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/nn.html#nce_loss" target="_blank" rel="external">官方文档</a>介绍,以及<a href="https://github.com/tensorflow/tensorflow/blob/e55574f28257bdacd744dcdba86c839e661b1b2a/tensorflow/python/ops/nn_impl.py" target="_blank" rel="external">tensorflow的源码</a>,</p>
<p>nce是为了加快多分类的速度，多分类下需要对每个可能类计算概率（100万个每个样本就得计算100万个概率），而在nce中，可以随机选几个类，计算概率，然后使用逻辑函数进行计算其他类别的概率(具体操作未知)。</p>
<p>值得注意的几点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">1,num_true大于1，即预测目标的概率和为1；</div><div class="line">2，参数详解：</div><div class="line">weights,[num_classes,dim]的张量，dim是样本的特征数量，</div><div class="line">biases，[num_classes]的张量，</div><div class="line">inputs,[batch_size,dim]的张量，前向激活网络的输入，喂入模型的训练数据，中心词</div><div class="line">labels，[batch_size,num_true],int64,目标类，目标词，</div><div class="line">num_sampled,每个batch中随机选的类数量，在word2vec中即随机选的负样本数（不是中心词本身），</div><div class="line">num_classes,所有可能的类别数量，</div><div class="line">num_true,每个训练样本的目标类别数（真实的）</div><div class="line">sampled_values，采样的方法，</div><div class="line">remove_accidental_hits，样本如果与目标类一致是否删除，设计到计算 loss的方法。</div><div class="line"></div><div class="line">其中inputs与labels是配对关系，</div><div class="line"></div><div class="line">3，返回[batch_size,d]为的nce损失值，是一个向量。</div></pre></td></tr></table></figure></p>
<p>sigmoid_cross_entropy_with_logits，多目标，可属于多个目标，<br>softmax_cross_entropy_with_logits，多分类，结果只能属于一个类，</p>
<h3 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h3><p>总的步骤，样本预处理生成，设置输入和输出，设置参数，设置损失函数，分配喂入数据，</p>
<ul>
<li>样本预处理生成</li>
</ul>
<p>word2vec在skip-model中的样本是词对，center_word–&gt;target_word,</p>
<ul>
<li>1，读取训练集，是一个（或多个）文件的文本语料，将这些语料读取进一个list中；</li>
<li>2，把list中的词进行词频统计，然后递减排序，做成一个word&lt;–&gt;index_id对，即把词映射成id，</li>
<li>3，根据word&lt;–&gt;index_id对，得到词典，词与id的映射，id与词的映射关系，</li>
<li>4，依照skip数，生成样本，迭代的，做法有很多种，从list（步骤1得到的）依次取一个词，然后分别从该词的前后[1-skip]中随机取一个词作为目标词，这样就组成了一个样本center_word–&gt;target_word，而词也顺便转成id（步骤2和3得到的映射词典）。随机是避免陷入局部最优。</li>
<li>5，根据4，得到一系列的样本，即list[center_word–&gt;target_word],然后可以做批次取样本，喂入模型。</li>
</ul>
<ul>
<li>设置输入和输出</li>
</ul>
<p>模型的输入是中心词，输出是目标词，采用placeholders，</p>
<p>center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name=’center_words’)<br>target_words = tf.placeholder(tf.int32,shape=[BATCH_SIZE,1],name=’target_words’)</p>
<p>设置全量的词嵌入矩阵，<br>embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name=’embed_matrix’)</p>
<ul>
<li>设置参数</li>
</ul>
<p>采取一层网络，参数w和b，<br>nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],stddev=1.0 / (EMBED_SIZE ** 0.5)), name=’nce_weight’)<br>nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name=’nce_bias’)</p>
<ul>
<li>损失函数</li>
</ul>
<p>先得到批次的向量，<br>embed = tf.nn.embedding_lookup(embed_matrix,center_words,name=’embed’)</p>
<p>tf.nn.embedding_lookup，根据input_ids中的id，寻找embedding中的对应向量（一行，从0开始计数）然后组成新的矩阵。</p>
<p>损失函数计算，<br>loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,<br>                                            biases=nce_bias,<br>                                            labels=target_words,<br>                                            inputs=embed,<br>                                            num_sampled=NUM_SAMPLED,#负样本数<br>                                            num_classes=VOCAB_SIZE), name=’loss’)</p>
<p>优化器，一般是批梯度下降法，<br>optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)</p>
<ul>
<li>喂入数据，迭代训练</li>
</ul>
<p>从样本预处理中，读取每个批次的样本，喂入模型中，</p>
<pre><code>with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    total_loss = 0.0 #总的loss
    writer = tf.summary.FileWriter(&apos;.my_graph/no_frills/&apos;, sess.graph)
    for index in range(NUM_TRAIN_STEPS):
        centers, targets = next(batch_gen) #已经生成的样本集合，不断循环取，
        loss_batch,_ = sess.run([loss,optimizer],feed_dict={center_words:centers,target_words:targets})

        total_loss += loss_batch
        if (index + 1) % SKIP_STEP == 0:
            print(&apos;Average loss at step {}: {:5.1f}&apos;.format(index, total_loss / SKIP_STEP))
            total_loss = 0.0
    writer.close()
</code></pre><ul>
<li>结果词向量</li>
</ul>
<p>最后训练完，把词向量保存，是embed_matrix;embed_matrix.eval()是numpy.ndarray类型。所以，不管训练用的多少层，只需记录最开始的词向量矩阵即可，</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>通过步骤的分解，整体的实现如下所示，含预处理和训练实现，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"></div><div class="line">from collections import Counter</div><div class="line">import random</div><div class="line">import os</div><div class="line">import sys</div><div class="line">sys.path.append(<span class="string">'..'</span>)</div><div class="line">import zipfile</div><div class="line"></div><div class="line">import numpy as np</div><div class="line">from six.moves import urllib</div><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">import utils</div><div class="line"></div><div class="line"><span class="comment"># Parameters for downloading data</span></div><div class="line">DOWNLOAD_URL = <span class="string">'http://mattmahoney.net/dc/'</span></div><div class="line">EXPECTED_BYTES = 31344016</div><div class="line">DATA_FOLDER = <span class="string">'data/'</span></div><div class="line">FILE_NAME = <span class="string">'text8'</span></div><div class="line"></div><div class="line">def download(file_name, expected_bytes):</div><div class="line">    <span class="string">""</span><span class="string">" Download the dataset text8 if it's not already downloaded "</span><span class="string">""</span></div><div class="line">    file_path = DATA_FOLDER + file_name</div><div class="line">    <span class="keyword">if</span> os.path.exists(file_path):</div><div class="line">        <span class="built_in">print</span>(<span class="string">"Dataset ready"</span>)</div><div class="line">        <span class="built_in">return</span> file_path</div><div class="line">    <span class="built_in">print</span>(DOWNLOAD_URL + file_name)</div><div class="line">    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)</div><div class="line"></div><div class="line">    file_stat = os.stat(file_path)</div><div class="line">    <span class="keyword">if</span> file_stat.st_size == expected_bytes:</div><div class="line">        <span class="built_in">print</span>(<span class="string">'Successfully downloaded the file'</span>, file_name)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        raise Exception(<span class="string">'File '</span> + file_name +</div><div class="line">                        <span class="string">' might be corrupted. You should try downloading it with a browser.'</span>)</div><div class="line">    <span class="built_in">return</span> file_path</div><div class="line"></div><div class="line">def read_data(file_path):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line">    大约有17,005,207词（含标点符号）</div><div class="line">    把所有的词读进一个list中，英文是空格分割，含标点符号，</div><div class="line">    "<span class="string">""</span></div><div class="line">    with zipfile.ZipFile(file_path) as f:</div><div class="line">        words = tf.compat.as_str(f.read(f.namelist()[0])).split()</div><div class="line">        <span class="comment"># tf.compat.as_str() 输入的转换成string</span></div><div class="line">    <span class="built_in">return</span> words</div><div class="line"></div><div class="line">def build_vocab(words, vocab_size):</div><div class="line">    <span class="string">""</span><span class="string">" Build vocabulary of VOCAB_SIZE most frequent words</span></div><div class="line">    建立字典，给定的vocab_size大小，建立；按照频率选取前vocab_size个词，</div><div class="line">    返回词的索引，数组和字典，</div><div class="line">    "<span class="string">""</span></div><div class="line">    dictionary = dict()</div><div class="line">    count = [(<span class="string">'UNK'</span>, -1)]</div><div class="line">    count.extend(Counter(words).most_common(vocab_size - 1))</div><div class="line">    index = 0</div><div class="line">    utils.make_dir(<span class="string">'processed'</span>)</div><div class="line">    with open(<span class="string">'processed/vocab_1000.tsv'</span>, <span class="string">"w"</span>) as f:</div><div class="line">        <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</div><div class="line">            dictionary[word] = index</div><div class="line">            <span class="keyword">if</span> index &lt; 1000:</div><div class="line">                f.write(word + <span class="string">"\n"</span>)</div><div class="line">            index += 1</div><div class="line">    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</div><div class="line">    <span class="built_in">return</span> dictionary, index_dictionary</div><div class="line"></div><div class="line">def convert_words_to_index(words, dictionary):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line">    把词替换成词典中的索引</div><div class="line">    "<span class="string">""</span></div><div class="line">    <span class="built_in">return</span> [dictionary[word] <span class="keyword">if</span> word <span class="keyword">in</span> dictionary <span class="keyword">else</span> 0 <span class="keyword">for</span> word <span class="keyword">in</span> words]</div><div class="line"></div><div class="line">def generate_sample(index_words, context_window_size):</div><div class="line">    <span class="string">""</span><span class="string">"</span></div><div class="line">    skip-gram模型，生成训练对，</div><div class="line"></div><div class="line">     "<span class="string">""</span></div><div class="line">    <span class="keyword">for</span> index, center <span class="keyword">in</span> enumerate(index_words):</div><div class="line">        context = random.randint(1, context_window_size)</div><div class="line">        <span class="comment"># 随机选择中心词的前一个后一个，还是前几个后几个，</span></div><div class="line">        <span class="keyword">for</span> target <span class="keyword">in</span> index_words[max(0, index - context): index]:</div><div class="line">            yield center, target</div><div class="line">        <span class="comment"># get a random target after the center wrod</span></div><div class="line">        <span class="keyword">for</span> target <span class="keyword">in</span> index_words[index + 1: index + context + 1]:</div><div class="line">            yield center, target</div><div class="line"></div><div class="line">def get_batch(iterator, batch_size):</div><div class="line">    <span class="string">""</span><span class="string">" Group a numerical stream into batches and yield them as Numpy arrays. "</span><span class="string">""</span></div><div class="line">    <span class="keyword">while</span> True:</div><div class="line">        center_batch = np.zeros(batch_size, dtype=np.int32)</div><div class="line">        target_batch = np.zeros([batch_size, 1])</div><div class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(batch_size):</div><div class="line">            center_batch[index], target_batch[index] = next(iterator)</div><div class="line">            <span class="comment">#next是从迭代器中挨个取值，这里是词对，生成的词对</span></div><div class="line">            <span class="comment">#中心词索引--&gt;前skip个（后skip个）词索引，这样的对，</span></div><div class="line">        yield center_batch, target_batch</div><div class="line"></div><div class="line">def process_data(vocab_size, batch_size, skip_window):</div><div class="line">    <span class="comment">#file_path = download(FILE_NAME, EXPECTED_BYTES)</span></div><div class="line">    file_path = <span class="string">"E:/workspace/DeepLearnings/tensorflow-learning/data/text8.zip"</span></div><div class="line">    <span class="built_in">print</span>(file_path)</div><div class="line">    words = read_data(file_path)<span class="comment">#读取训练所有词，存储在一个list中，</span></div><div class="line">    dictionary, _ = build_vocab(words, vocab_size)<span class="comment">#建立词典，根据指定的词典大小，词典的构建依赖于词在训练样本中的频率</span></div><div class="line">    index_words = convert_words_to_index(words, dictionary)<span class="comment">#得到词典，把训练集替换成索引，即文字--&gt;数字</span></div><div class="line">    del words <span class="comment"># 词存入内存</span></div><div class="line">    single_gen = generate_sample(index_words, skip_window)<span class="comment">#获取样本，根据中心词选取前后skip个词，构成词对，</span></div><div class="line">    <span class="built_in">return</span> get_batch(single_gen, batch_size)</div><div class="line"></div><div class="line">def get_index_vocab(vocab_size):</div><div class="line">    file_path = download(FILE_NAME, EXPECTED_BYTES)</div><div class="line">    words = read_data(file_path)</div><div class="line">    <span class="built_in">return</span> build_vocab(words, vocab_size)</div></pre></td></tr></table></figure></p>
<p>训练的实现，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#coding=utf-8</span></div><div class="line"></div><div class="line"><span class="string">""</span><span class="string">" The mo frills implementation of word2vec skip-gram model using NCE loss.</span></div><div class="line">Author: Chip Huyen</div><div class="line">Prepared for the class CS 20SI: "TensorFlow <span class="keyword">for</span> Deep Learning Research<span class="string">"</span></div><div class="line">cs20si.stanford.edu</div><div class="line">"<span class="string">""</span></div><div class="line"></div><div class="line">import os</div><div class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></div><div class="line"></div><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">from tensorflow.contrib.tensorboard.plugins import projector</div><div class="line"></div><div class="line">from process_data import process_data</div><div class="line"></div><div class="line">VOCAB_SIZE = 50000 <span class="comment">#词典大小</span></div><div class="line">BATCH_SIZE = 128 <span class="comment"># 每个批次的大小，即每个批次包含的样本量</span></div><div class="line">EMBED_SIZE = 128 <span class="comment"># 每个词的嵌入向量大小，即词向量维度</span></div><div class="line">SKIP_WINDOW = 1 <span class="comment"># 窗口，即中心词调几个词</span></div><div class="line">NUM_SAMPLED = 64    <span class="comment"># 抽样时取的负样本个数</span></div><div class="line">LEARNING_RATE = 1.0</div><div class="line">NUM_TRAIN_STEPS = 20000</div><div class="line">SKIP_STEP = 2000 <span class="comment"># 优化loss，训练多少次</span></div><div class="line"></div><div class="line">def word2vec(batch_gen):</div><div class="line">    <span class="string">""</span><span class="string">" Build the graph for word2vec model and train it "</span><span class="string">""</span></div><div class="line">    <span class="comment"># 使用placeholders设置输入和输出，即中心词和目标词</span></div><div class="line">    with tf.name_scope(<span class="string">'data'</span>):</div><div class="line">        center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name=<span class="string">'center_words'</span>)</div><div class="line">        target_words = tf.placeholder(tf.int32,shape=[BATCH_SIZE,1],name=<span class="string">'target_words'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 词嵌入矩阵[词典大小,词向量大小]，</span></div><div class="line">    with tf.name_scope(<span class="string">'mbedding_matrix'</span>):</div><div class="line">        embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name=<span class="string">'embed_matrix'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 模型</span></div><div class="line">    <span class="comment"># tf.nn.embedding_lookup，根据input_ids中的id，寻找embedding中的对应向量（一行，从0开始计数）然后组成新的矩阵，</span></div><div class="line">    <span class="comment"># embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')</span></div><div class="line">    <span class="comment"># 开始训练，设置w和b，</span></div><div class="line">    with tf.name_scope(<span class="string">'loss'</span>):</div><div class="line">        embed = tf.nn.embedding_lookup(embed_matrix,center_words,name=<span class="string">'embed'</span>)</div><div class="line">        <span class="comment"># Step 4: construct variables for NCE loss</span></div><div class="line">        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],stddev=1.0 / (EMBED_SIZE ** 0.5)), name=<span class="string">'nce_weight'</span>)</div><div class="line">        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name=<span class="string">'nce_bias'</span>)</div><div class="line">        <span class="comment"># nce损失函数</span></div><div class="line">        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,</div><div class="line">                                            biases=nce_bias,</div><div class="line">                                            labels=target_words,</div><div class="line">                                            inputs=embed,</div><div class="line">                                            num_sampled=NUM_SAMPLED,<span class="comment">#负样本数</span></div><div class="line">                                            num_classes=VOCAB_SIZE), name=<span class="string">'loss'</span>)</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># 优化</span></div><div class="line">    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)</div><div class="line"></div><div class="line">    <span class="comment">#迭代训练</span></div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">        total_loss = 0.0 <span class="comment">#总的loss</span></div><div class="line">        writer = tf.summary.FileWriter(<span class="string">'.my_graph/no_frills/'</span>, sess.graph)</div><div class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(NUM_TRAIN_STEPS):</div><div class="line">            centers, targets = next(batch_gen)</div><div class="line">            loss_batch,_ = sess.run([loss,optimizer],feed_dict=&#123;center_words:centers,target_words:targets&#125;)</div><div class="line"></div><div class="line">            total_loss += loss_batch</div><div class="line">            <span class="keyword">if</span> (index + 1) % SKIP_STEP == 0:</div><div class="line">                <span class="built_in">print</span>(<span class="string">'Average loss at step &#123;&#125;: &#123;:5.1f&#125;'</span>.format(index, total_loss / SKIP_STEP))</div><div class="line">                total_loss = 0.0</div><div class="line">        writer.close()</div><div class="line">        <span class="comment">#最后训练完是embed_matrix;embed_matrix.eval()是numpy.ndarray类型</span></div><div class="line">def main():</div><div class="line">    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)</div><div class="line">    word2vec(batch_gen)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure></p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>把词转为词向量后，即把中文词转换成了数字，并且是一维向量，可以在很多数学模型中使用。</p>
<p>比如，作为其他模型的特征输入；向量之间计算词的相似性；通过词计算短语或句子的向量（当然有其他模型训练短语和句子的向量）；</p>
<p>总之，词向量的得到，可以作为其他应用的辅助或基础，另外，词向量的作用与lda-主题模型同理，可以得到词的向量表示。然而，主题模型，可以把词形象的标记在不同主题的概率，而词向量只是在同一空间内，把词进行重新分置，视具体的应用场景选择lda的向量表示还是词向量的表示。</p>
<h2 id="模型复用"><a href="#模型复用" class="headerlink" title="模型复用"></a>模型复用</h2><p>上述训练word2vec，可以看出代码没有类，使得代码在使用上欠缺复用性，下面把训练的模型做改变，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">class SkipGramModel:</div><div class="line">    def __init__(self, params):</div><div class="line">       pass</div><div class="line">    def _create_placeholders(self):</div><div class="line">       pass</div><div class="line">    def _create_embedding(self):</div><div class="line">       pass</div><div class="line">    def _create_loss(self):</div><div class="line">       pass</div><div class="line">    def _create_optimizer(self):</div><div class="line">       pass</div><div class="line">    def _create_summaries(self):</div><div class="line">       pass</div><div class="line">    def build_graph(self):</div><div class="line">       pass</div></pre></td></tr></table></figure>
<p>之后，把相关的逻辑填充在类的方法里即可。<a href="https://github.com/chlyzzo/DeepLearnings/blob/master/tensorflow-learning/word2vec/04_word2vec_visualize.py" target="_blank" rel="external">详细的代码实现</a>.</p>
  
	</div>
		<footer class="article-footer clearfix">

	<div class="article-share" id="share">
	
	  <div data-url="https://chlyzzo.github.io/2017/09/28/tensorflow学习-word2vec/" data-title="tensorflow学习-word2vec | 山上掏金" data-tsina="1724571293" class="share clearfix">
	  </div>
	
	</div>


</footer>

   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/10/10/tensorflow-卷积/" title="tensorflow-卷积">
  <strong>上一篇：</strong><br/>
  <span>
  tensorflow-卷积</span>
</a>
</div>


<div class="next">
<a href="/2017/09/21/通过python，java，spark，hive以post提交数据/"  title="通过python，java，spark，hive以post提交数据">
 <strong>下一篇：</strong><br/> 
 <span>通过python，java，spark，hive以post提交数据
</span>
</a>
</div>

</nav>

	

</div>  
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> 分享即是收获，动手后才是自己的. <br/>
			</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1724571293" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/chlyzzo" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:rimin515@sina.cn" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by © 2017 
		
		<a href="/about" target="_blank" title="wenxi">wenxi</a>
		
		
		</p>

</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>









<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?81e8b61e30a0723ad6270902dbcf0bb6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>




<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
</html>
